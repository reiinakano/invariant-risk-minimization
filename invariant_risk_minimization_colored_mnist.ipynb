{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "invariant-risk-minimization-colored-mnist.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reiinakano/invariant-risk-minimization/blob/master/invariant_risk_minimization_colored_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLDC06POu-Z5",
        "colab_type": "text"
      },
      "source": [
        "# Invariant Risk Minimization\n",
        "\n",
        "This is an attempt to reproduce the \"Colored MNIST\" experiments from the\n",
        "paper [Invariant Risk Minimization](https://arxiv.org/abs/1907.02893)\n",
        "by Arjovsky, et. al."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sopHPgEhu4Jo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import grad\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "import torchvision.datasets.utils as dataset_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_u5rBUnvdxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkTMK-oJveGg",
        "colab_type": "text"
      },
      "source": [
        "## Prepare the colored MNIST dataset\n",
        "\n",
        "We define three environments (two training, one test) by randomly splitting the MNIST dataset in thirds and transforming each example as follows:\n",
        "1. Assign a binary label y to the image based on the digit: y = 0 for digits 0-4\n",
        "and y = 1 for digits 5-9.\n",
        "2. Flip the label with 25% probability.\n",
        "3. Color the image either red or green according to its (possibly flipped) label.\n",
        "4. Flip the color with a probability e that depends on the environment: 20% in\n",
        "the first training environment, 10% in the second training environment, and\n",
        "90% in the test environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knP-xNzavgAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def color_grayscale_arr(arr, red=True):\n",
        "  \"\"\"Converts grayscale image to either red or green\"\"\"\n",
        "  assert arr.ndim == 2\n",
        "  dtype = arr.dtype\n",
        "  h, w = arr.shape\n",
        "  arr = np.reshape(arr, [h, w, 1])\n",
        "  if red:\n",
        "    arr = np.concatenate([arr,\n",
        "                          np.zeros((h, w, 2), dtype=dtype)], axis=2)\n",
        "  else:\n",
        "    arr = np.concatenate([np.zeros((h, w, 1), dtype=dtype),\n",
        "                          arr,\n",
        "                          np.zeros((h, w, 1), dtype=dtype)], axis=2)\n",
        "  return arr\n",
        "\n",
        "\n",
        "class ColoredMNIST(datasets.VisionDataset):\n",
        "  \"\"\"\n",
        "  Colored MNIST dataset for testing IRM. Prepared using procedure from https://arxiv.org/pdf/1907.02893.pdf\n",
        "\n",
        "  Args:\n",
        "    root (string): Root directory of dataset where ``ColoredMNIST/*.pt`` will exist.\n",
        "    env (string): Which environment to load. Must be 1 of 'train1', 'train2', 'test', or 'all_train'.\n",
        "    transform (callable, optional): A function/transform that  takes in an PIL image\n",
        "      and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "    target_transform (callable, optional): A function/transform that takes in the\n",
        "      target and transforms it.\n",
        "  \"\"\"\n",
        "  def __init__(self, root='./data', env='train1', transform=None, target_transform=None):\n",
        "    super(ColoredMNIST, self).__init__(root, transform=transform,\n",
        "                                target_transform=target_transform)\n",
        "\n",
        "    self.prepare_colored_mnist()\n",
        "    if env in ['train1', 'train2', 'test']:\n",
        "      self.data_label_tuples = torch.load(os.path.join(self.root, 'ColoredMNIST', env) + '.pt')\n",
        "    elif env == 'all_train':\n",
        "      self.data_label_tuples = torch.load(os.path.join(self.root, 'ColoredMNIST', 'train1.pt')) + \\\n",
        "                               torch.load(os.path.join(self.root, 'ColoredMNIST', 'train2.pt'))\n",
        "    else:\n",
        "      raise RuntimeError(f'{env} env unknown. Valid envs are train1, train2, test, and all_train')\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        index (int): Index\n",
        "\n",
        "    Returns:\n",
        "        tuple: (image, target) where target is index of the target class.\n",
        "    \"\"\"\n",
        "    img, target = self.data_label_tuples[index]\n",
        "\n",
        "    if self.transform is not None:\n",
        "      img = self.transform(img)\n",
        "\n",
        "    if self.target_transform is not None:\n",
        "      target = self.target_transform(target)\n",
        "\n",
        "    return img, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data_label_tuples)\n",
        "\n",
        "  def prepare_colored_mnist(self):\n",
        "    colored_mnist_dir = os.path.join(self.root, 'ColoredMNIST')\n",
        "    if os.path.exists(os.path.join(colored_mnist_dir, 'train1.pt')) \\\n",
        "        and os.path.exists(os.path.join(colored_mnist_dir, 'train2.pt')) \\\n",
        "        and os.path.exists(os.path.join(colored_mnist_dir, 'test.pt')):\n",
        "      print('Colored MNIST dataset already exists')\n",
        "      return\n",
        "\n",
        "    print('Preparing Colored MNIST')\n",
        "    train_mnist = datasets.mnist.MNIST(self.root, train=True, download=True)\n",
        "\n",
        "    train1_set = []\n",
        "    train2_set = []\n",
        "    test_set = []\n",
        "    for idx, (im, label) in enumerate(train_mnist):\n",
        "      if idx % 10000 == 0:\n",
        "        print(f'Converting image {idx}/{len(train_mnist)}')\n",
        "      im_array = np.array(im)\n",
        "\n",
        "      # Assign a binary label y to the image based on the digit\n",
        "      binary_label = 0 if label < 5 else 1\n",
        "\n",
        "      # Flip label with 25% probability\n",
        "      if np.random.uniform() < 0.25:\n",
        "        binary_label = binary_label ^ 1\n",
        "\n",
        "      # Color the image either red or green according to its possibly flipped label\n",
        "      color_red = binary_label == 0\n",
        "\n",
        "      # Flip the color with a probability e that depends on the environment\n",
        "      if idx < 20000:\n",
        "        # 20% in the first training environment\n",
        "        if np.random.uniform() < 0.2:\n",
        "          color_red = not color_red\n",
        "      elif idx < 40000:\n",
        "        # 10% in the first training environment\n",
        "        if np.random.uniform() < 0.1:\n",
        "          color_red = not color_red\n",
        "      else:\n",
        "        # 90% in the test environment\n",
        "        if np.random.uniform() < 0.9:\n",
        "          color_red = not color_red\n",
        "\n",
        "      colored_arr = color_grayscale_arr(im_array, red=color_red)\n",
        "\n",
        "      if idx < 20000:\n",
        "        train1_set.append((Image.fromarray(colored_arr), binary_label))\n",
        "      elif idx < 40000:\n",
        "        train2_set.append((Image.fromarray(colored_arr), binary_label))\n",
        "      else:\n",
        "        test_set.append((Image.fromarray(colored_arr), binary_label))\n",
        "\n",
        "      # Debug\n",
        "      # print('original label', type(label), label)\n",
        "      # print('binary label', binary_label)\n",
        "      # print('assigned color', 'red' if color_red else 'green')\n",
        "      # plt.imshow(colored_arr)\n",
        "      # plt.show()\n",
        "      # break\n",
        "\n",
        "    dataset_utils.makedir_exist_ok(colored_mnist_dir)\n",
        "    torch.save(train1_set, os.path.join(colored_mnist_dir, 'train1.pt'))\n",
        "    torch.save(train2_set, os.path.join(colored_mnist_dir, 'train2.pt'))\n",
        "    torch.save(test_set, os.path.join(colored_mnist_dir, 'test.pt'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOnjIjK8q7UJ",
        "colab_type": "text"
      },
      "source": [
        "### Plot the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwUoQZCyvs6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_dataset_digits(dataset):\n",
        "  fig = plt.figure(figsize=(13, 8))\n",
        "  columns = 6\n",
        "  rows = 3\n",
        "  # ax enables access to manipulate each of subplots\n",
        "  ax = []\n",
        "\n",
        "  for i in range(columns * rows):\n",
        "    img, label = dataset[i]\n",
        "    # create subplot and append to ax\n",
        "    ax.append(fig.add_subplot(rows, columns, i + 1))\n",
        "    ax[-1].set_title(\"Label: \" + str(label))  # set title\n",
        "    plt.imshow(img)\n",
        "\n",
        "  plt.show()  # finally, render the plot\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11YqxmhjrSMi",
        "colab_type": "text"
      },
      "source": [
        "Plotting the train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDh4qJS_rQwK",
        "colab_type": "code",
        "outputId": "5988b695-e8be-4fd0-a99b-fc11f1daaa06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        }
      },
      "source": [
        "train1_set = ColoredMNIST(root='./data', env='train1')\n",
        "plot_dataset_digits(train1_set)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Colored MNIST dataset already exists\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvoAAAHKCAYAAAByje/lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAQIb+P2RD2LMlNAy8AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 936x576 with 18 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhsLWjOCrqWx",
        "colab_type": "text"
      },
      "source": [
        "Plotting the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdmmW48srQyk",
        "colab_type": "code",
        "outputId": "4d64bea8-9015-4e97-ae4c-03865a3ab538",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        }
      },
      "source": [
        "test_set = ColoredMNIST(root='./data', env='test')\n",
        "plot_dataset_digits(test_set)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Colored MNIST dataset already exists\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvoAAAHKCAYAAAByje/lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAECC/h9DfeyVK8HFVgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 936x576 with 18 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo3Whs4hrr-C",
        "colab_type": "text"
      },
      "source": [
        "Notice how the correlation between color and label are reversed in the train and test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jleIZ9vNv5rV",
        "colab_type": "text"
      },
      "source": [
        "## Define neural network\n",
        "\n",
        "The paper uses an MLP but a Convnet works fine too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hYJRewnv80x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.fc1 = nn.Linear(3 * 28 * 28, 512)\n",
        "    self.fc2 = nn.Linear(512, 512)\n",
        "    self.fc3 = nn.Linear(512, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 3 * 28 * 28)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    logits = self.fc3(x).flatten()\n",
        "    return logits\n",
        "\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 20, 5, 1)\n",
        "    self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "    self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
        "    self.fc2 = nn.Linear(500, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = F.max_pool2d(x, 2, 2)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = F.max_pool2d(x, 2, 2)\n",
        "    x = x.view(-1, 4 * 4 * 50)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    logits = self.fc2(x).flatten()\n",
        "    return logits\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj5Q6UTlwGM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il3cATYxwIyP",
        "colab_type": "text"
      },
      "source": [
        "## Test ERM as a baseline\n",
        "\n",
        "Using ERM as a baseline, we expect to train a neural network that uses color instead of the actual digit to classify, completely failing on the test set when the colors are switched."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4qZtXx_weBb",
        "colab_type": "code",
        "outputId": "3b4a116e-de35-446b-b426-ef83f0dfaa67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def test_model(model, device, test_loader, set_name=\"test set\"):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data, target = data.to(device), target.to(device).float()\n",
        "      output = model(data)\n",
        "      test_loss += F.binary_cross_entropy_with_logits(output, target, reduction='sum').item()  # sum up batch loss\n",
        "      pred = torch.where(torch.gt(output, torch.Tensor([0.0]).to(device)),\n",
        "                         torch.Tensor([1.0]).to(device),\n",
        "                         torch.Tensor([0.0]).to(device))  # get the index of the max log-probability\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "\n",
        "  print('\\nPerformance on {}: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "    set_name, test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "  return 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "\n",
        "def erm_train(model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = data.to(device), target.to(device).float()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.binary_cross_entropy_with_logits(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % 10 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "               100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def train_and_test_erm():\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "  kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "  all_train_loader = torch.utils.data.DataLoader(\n",
        "    ColoredMNIST(root='./data', env='all_train',\n",
        "                 transform=transforms.Compose([\n",
        "                     transforms.ToTensor(),\n",
        "                     transforms.Normalize((0.1307, 0.1307, 0.), (0.3081, 0.3081, 0.3081))\n",
        "                   ])),\n",
        "    batch_size=64, shuffle=True, **kwargs)\n",
        "\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "    ColoredMNIST(root='./data', env='test', transform=transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.1307, 0.1307, 0.), (0.3081, 0.3081, 0.3081))\n",
        "    ])),\n",
        "    batch_size=1000, shuffle=True, **kwargs)\n",
        "\n",
        "  model = ConvNet().to(device)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "  for epoch in range(1, 2):\n",
        "    erm_train(model, device, all_train_loader, optimizer, epoch)\n",
        "    test_model(model, device, all_train_loader, set_name='train set')\n",
        "    test_model(model, device, test_loader)\n",
        "\n",
        "train_and_test_erm()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Colored MNIST dataset already exists\n",
            "Colored MNIST dataset already exists\n",
            "Train Epoch: 1 [0/40000 (0%)]\tLoss: 0.696104\n",
            "Train Epoch: 1 [640/40000 (2%)]\tLoss: 0.689937\n",
            "Train Epoch: 1 [1280/40000 (3%)]\tLoss: 0.598053\n",
            "Train Epoch: 1 [1920/40000 (5%)]\tLoss: 0.604109\n",
            "Train Epoch: 1 [2560/40000 (6%)]\tLoss: 0.556638\n",
            "Train Epoch: 1 [3200/40000 (8%)]\tLoss: 0.370532\n",
            "Train Epoch: 1 [3840/40000 (10%)]\tLoss: 0.394240\n",
            "Train Epoch: 1 [4480/40000 (11%)]\tLoss: 0.347356\n",
            "Train Epoch: 1 [5120/40000 (13%)]\tLoss: 0.341673\n",
            "Train Epoch: 1 [5760/40000 (14%)]\tLoss: 0.432643\n",
            "Train Epoch: 1 [6400/40000 (16%)]\tLoss: 0.418959\n",
            "Train Epoch: 1 [7040/40000 (18%)]\tLoss: 0.465067\n",
            "Train Epoch: 1 [7680/40000 (19%)]\tLoss: 0.480257\n",
            "Train Epoch: 1 [8320/40000 (21%)]\tLoss: 0.388918\n",
            "Train Epoch: 1 [8960/40000 (22%)]\tLoss: 0.399111\n",
            "Train Epoch: 1 [9600/40000 (24%)]\tLoss: 0.534256\n",
            "Train Epoch: 1 [10240/40000 (26%)]\tLoss: 0.421795\n",
            "Train Epoch: 1 [10880/40000 (27%)]\tLoss: 0.390130\n",
            "Train Epoch: 1 [11520/40000 (29%)]\tLoss: 0.410140\n",
            "Train Epoch: 1 [12160/40000 (30%)]\tLoss: 0.330126\n",
            "Train Epoch: 1 [12800/40000 (32%)]\tLoss: 0.313489\n",
            "Train Epoch: 1 [13440/40000 (34%)]\tLoss: 0.297960\n",
            "Train Epoch: 1 [14080/40000 (35%)]\tLoss: 0.658392\n",
            "Train Epoch: 1 [14720/40000 (37%)]\tLoss: 0.328783\n",
            "Train Epoch: 1 [15360/40000 (38%)]\tLoss: 0.532167\n",
            "Train Epoch: 1 [16000/40000 (40%)]\tLoss: 0.352024\n",
            "Train Epoch: 1 [16640/40000 (42%)]\tLoss: 0.458649\n",
            "Train Epoch: 1 [17280/40000 (43%)]\tLoss: 0.335565\n",
            "Train Epoch: 1 [17920/40000 (45%)]\tLoss: 0.408704\n",
            "Train Epoch: 1 [18560/40000 (46%)]\tLoss: 0.465430\n",
            "Train Epoch: 1 [19200/40000 (48%)]\tLoss: 0.483388\n",
            "Train Epoch: 1 [19840/40000 (50%)]\tLoss: 0.387358\n",
            "Train Epoch: 1 [20480/40000 (51%)]\tLoss: 0.366326\n",
            "Train Epoch: 1 [21120/40000 (53%)]\tLoss: 0.417196\n",
            "Train Epoch: 1 [21760/40000 (54%)]\tLoss: 0.354960\n",
            "Train Epoch: 1 [22400/40000 (56%)]\tLoss: 0.377138\n",
            "Train Epoch: 1 [23040/40000 (58%)]\tLoss: 0.271766\n",
            "Train Epoch: 1 [23680/40000 (59%)]\tLoss: 0.449814\n",
            "Train Epoch: 1 [24320/40000 (61%)]\tLoss: 0.304885\n",
            "Train Epoch: 1 [24960/40000 (62%)]\tLoss: 0.295312\n",
            "Train Epoch: 1 [25600/40000 (64%)]\tLoss: 0.420703\n",
            "Train Epoch: 1 [26240/40000 (66%)]\tLoss: 0.337034\n",
            "Train Epoch: 1 [26880/40000 (67%)]\tLoss: 0.456587\n",
            "Train Epoch: 1 [27520/40000 (69%)]\tLoss: 0.512981\n",
            "Train Epoch: 1 [28160/40000 (70%)]\tLoss: 0.500018\n",
            "Train Epoch: 1 [28800/40000 (72%)]\tLoss: 0.340696\n",
            "Train Epoch: 1 [29440/40000 (74%)]\tLoss: 0.373836\n",
            "Train Epoch: 1 [30080/40000 (75%)]\tLoss: 0.335393\n",
            "Train Epoch: 1 [30720/40000 (77%)]\tLoss: 0.400599\n",
            "Train Epoch: 1 [31360/40000 (78%)]\tLoss: 0.444043\n",
            "Train Epoch: 1 [32000/40000 (80%)]\tLoss: 0.372643\n",
            "Train Epoch: 1 [32640/40000 (82%)]\tLoss: 0.447174\n",
            "Train Epoch: 1 [33280/40000 (83%)]\tLoss: 0.403641\n",
            "Train Epoch: 1 [33920/40000 (85%)]\tLoss: 0.339839\n",
            "Train Epoch: 1 [34560/40000 (86%)]\tLoss: 0.340390\n",
            "Train Epoch: 1 [35200/40000 (88%)]\tLoss: 0.353417\n",
            "Train Epoch: 1 [35840/40000 (90%)]\tLoss: 0.381741\n",
            "Train Epoch: 1 [36480/40000 (91%)]\tLoss: 0.431652\n",
            "Train Epoch: 1 [37120/40000 (93%)]\tLoss: 0.394919\n",
            "Train Epoch: 1 [37760/40000 (94%)]\tLoss: 0.359211\n",
            "Train Epoch: 1 [38400/40000 (96%)]\tLoss: 0.448928\n",
            "Train Epoch: 1 [39040/40000 (98%)]\tLoss: 0.322221\n",
            "Train Epoch: 1 [39680/40000 (99%)]\tLoss: 0.322353\n",
            "\n",
            "Performance on train set: Average loss: 0.4070, Accuracy: 33940/40000 (84.85%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.4814, Accuracy: 2053/20000 (10.27%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6wptoCqwjZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1-MgdGuwlCY",
        "colab_type": "text"
      },
      "source": [
        "## IRM\n",
        "\n",
        "After trying lots of hyperparameters and various tricks, I was able to \"sort of\"\n",
        "achieve the results in the paper. I say \"sort of\" because the training process\n",
        "is quite unstable and dependent on the random seed. A small percentage of runs may\n",
        "not converge to the paper-reported values (train accuracy > 70%, test accuracy > 60%)\n",
        "after a few tens of epochs.\n",
        "\n",
        "The most common failure case is when the gradient norm penalty term is weighted\n",
        "too highly relative to the ERM term. In this case,  converges to a function that \n",
        "returns the same value for all inputs. The classifier cannot recover from this point\n",
        "and the accuracy is stuck at 50% for all environments. This makes sense mathematically.\n",
        "If the intermediate representation is the same regardless of input, then *any*\n",
        "classifier is the ideal classifier, resulting in the gradient being 0.\n",
        "\n",
        "Another failure case is when the gradient norm penalty is too low and the\n",
        "optimization essentially acts as in ERM (train accuracy > 80%, test accuracy ~10%).\n",
        "\n",
        "The most important trick I used to get this to work is through scheduled \n",
        "increase of the gradient norm penalty weight.\n",
        "We start at 0 for the gradient norm penalty weight, essentially beginning as ERM,\n",
        "then slowly increase it per epoch.\n",
        "\n",
        "I use early stopping to stop training once the accuracy on all environments, \n",
        "including the test set, reach an acceptable value. Yes, stopping training based on \n",
        "performance on the test set is not good practice, but I could not\n",
        "find a principled way of stopping training by only observing performance on the\n",
        "training environments. One thing that might be needed when applying IRM to\n",
        "real-world datasets is to leave out a separate environment as a validation set,\n",
        "which we can use for early stopping. The downside is we'll need a minimum of 4\n",
        "environments to perform IRM (2 train, 1 validation, 1 test)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kio2CQOdwqA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_irm_penalty(losses, dummy):\n",
        "  g1 = grad(losses[0::2].mean(), dummy, create_graph=True)[0]\n",
        "  g2 = grad(losses[1::2].mean(), dummy, create_graph=True)[0]\n",
        "  return (g1 * g2).sum()\n",
        "\n",
        "\n",
        "def irm_train(model, device, train_loaders, optimizer, epoch):\n",
        "  model.train()\n",
        "\n",
        "  train_loaders = [iter(x) for x in train_loaders]\n",
        "\n",
        "  dummy_w = torch.nn.Parameter(torch.Tensor([1.0])).to(device)\n",
        "\n",
        "  batch_idx = 0\n",
        "  penalty_multiplier = min(epoch//2 * 10., 300.)\n",
        "  print(f'Using penalty multiplier {penalty_multiplier}')\n",
        "  while True:\n",
        "    optimizer.zero_grad()\n",
        "    error = 0\n",
        "    penalty = 0\n",
        "    for loader in train_loaders:\n",
        "      data, target = next(loader, (None, None))\n",
        "      if data is None:\n",
        "        return\n",
        "      data, target = data.to(device), target.to(device).float()\n",
        "      output = model(data)\n",
        "      loss_erm = F.binary_cross_entropy_with_logits(output * dummy_w, target, reduction='none')\n",
        "      penalty += compute_irm_penalty(loss_erm, dummy_w)\n",
        "      error += loss_erm.mean()\n",
        "    (error + penalty_multiplier * penalty).backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % 2 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tERM loss: {:.6f}\\tGrad penalty: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loaders[0].dataset),\n",
        "               100. * batch_idx / len(train_loaders[0]), error.item(), penalty.item()))\n",
        "      print('First 20 logits', output.data.cpu().numpy()[:20])\n",
        "\n",
        "    batch_idx += 1\n",
        "\n",
        "\n",
        "def train_and_test_irm():\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "  kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "  train1_loader = torch.utils.data.DataLoader(\n",
        "    ColoredMNIST(root='./data', env='train1',\n",
        "                 transform=transforms.Compose([\n",
        "                     transforms.ToTensor(),\n",
        "                     transforms.Normalize((0.1307, 0.1307, 0.), (0.3081, 0.3081, 0.3081))\n",
        "                   ])),\n",
        "    batch_size=2000, shuffle=True, **kwargs)\n",
        "\n",
        "  train2_loader = torch.utils.data.DataLoader(\n",
        "    ColoredMNIST(root='./data', env='train2',\n",
        "                 transform=transforms.Compose([\n",
        "                     transforms.ToTensor(),\n",
        "                     transforms.Normalize((0.1307, 0.1307, 0.), (0.3081, 0.3081, 0.3081))\n",
        "                   ])),\n",
        "    batch_size=2000, shuffle=True, **kwargs)\n",
        "\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "    ColoredMNIST(root='./data', env='test', transform=transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.1307, 0.1307, 0.), (0.3081, 0.3081, 0.3081))\n",
        "    ])),\n",
        "    batch_size=1000, shuffle=True, **kwargs)\n",
        "\n",
        "  model = ConvNet().to(device)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "  for epoch in range(1, 100):\n",
        "    irm_train(model, device, [train1_loader, train2_loader], optimizer, epoch)\n",
        "    train1_acc = test_model(model, device, train1_loader, set_name='train1 set')\n",
        "    train2_acc = test_model(model, device, train2_loader, set_name='train2 set')\n",
        "    test_acc = test_model(model, device, test_loader)\n",
        "    if train1_acc > 70 and train2_acc > 70 and test_acc > 60:\n",
        "      print('found acceptable values. stopping training.')\n",
        "      return\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXFWJLc-xm-F",
        "colab_type": "code",
        "outputId": "69f82e57-ce55-43f1-88c2-91687713333a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_and_test_irm()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Colored MNIST dataset already exists\n",
            "Colored MNIST dataset already exists\n",
            "Colored MNIST dataset already exists\n",
            "Using penalty multiplier 0.0\n",
            "Train Epoch: 1 [0/20000 (0%)]\tERM loss: 1.386654\tGrad penalty: 0.000000\n",
            "First 20 logits [ 0.02287975  0.10063981  0.01309153  0.02457361  0.01361204 -0.01184867\n",
            "  0.01361048  0.03210351  0.00901007 -0.01329951  0.03865903  0.07497786\n",
            "  0.03008205  0.0057271   0.02080771  0.03794264  0.05106601  0.01280301\n",
            "  0.05163205  0.0256582 ]\n",
            "Train Epoch: 1 [4000/20000 (20%)]\tERM loss: 1.004817\tGrad penalty: 0.035569\n",
            "First 20 logits [ 0.8356074   0.6580241  -1.0186672   0.40962037 -1.0270716   0.578809\n",
            "  0.87866914 -0.68669903 -0.9980491   0.6254485  -0.689821    0.61712223\n",
            "  0.62599576  0.6636586   0.4813903  -0.67412114 -0.65330505 -0.45696706\n",
            " -0.9860666  -0.84134835]\n",
            "Train Epoch: 1 [8000/20000 (40%)]\tERM loss: 0.918591\tGrad penalty: 0.101258\n",
            "First 20 logits [ 2.5872993 -3.2638505  2.6914477  2.577178  -2.9873114  2.1092696\n",
            "  2.2789958  2.750548   2.698901   2.4930081 -2.5574574 -2.3922033\n",
            " -3.2461488  1.0669559  2.0853171  2.5222576  2.6623983  2.1138291\n",
            "  2.6827185  1.979556 ]\n",
            "Train Epoch: 1 [12000/20000 (60%)]\tERM loss: 0.875273\tGrad penalty: 0.083676\n",
            "First 20 logits [ 2.855164   2.4007444  2.7966442  2.7710876  2.9624841 -1.3293486\n",
            " -1.450613  -1.4472975 -2.5100238  2.3642855  1.7840793  3.3425598\n",
            " -2.9119287 -2.240297   3.053767   2.5075786 -2.6867473  2.9264197\n",
            " -2.6040125 -2.4963953]\n",
            "Train Epoch: 1 [16000/20000 (80%)]\tERM loss: 0.860602\tGrad penalty: 0.019131\n",
            "First 20 logits [ 1.7410517 -1.7658596 -1.3926287 -1.2848235  1.4584298  1.8232888\n",
            " -1.4933516  1.433466   1.339468  -1.3464416  1.3898162  1.5569834\n",
            "  1.243078  -1.354601   1.8246306  1.7851177  1.5609771  1.6492314\n",
            "  1.5785908 -1.4699006]\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5030, Accuracy: 15958/20000 (79.79%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.3759, Accuracy: 18152/20000 (90.76%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.3110, Accuracy: 1956/20000 (9.78%)\n",
            "\n",
            "Using penalty multiplier 10.0\n",
            "Train Epoch: 2 [0/20000 (0%)]\tERM loss: 0.867327\tGrad penalty: 0.030754\n",
            "First 20 logits [ 1.1089131   1.100398    1.3829353   1.08253    -1.5677303  -1.2842112\n",
            "  1.1150447   1.1743013   1.3330775   1.1955205   1.2272108  -1.1932135\n",
            " -1.6621046   1.1128988   1.0654469   0.87824875 -1.1213733  -1.1736894\n",
            " -1.2804093   0.8639043 ]\n",
            "Train Epoch: 2 [4000/20000 (20%)]\tERM loss: 0.854759\tGrad penalty: 0.016504\n",
            "First 20 logits [-2.5582416 -2.720464  -2.2555966  1.2615063 -2.1587346  1.6038672\n",
            "  1.7701532 -2.0482564  1.4818183  1.192933  -1.965392  -2.640568\n",
            "  0.9882209  1.1960666 -2.3200245  1.3800272  1.0114665 -2.3115988\n",
            " -2.8629758  1.3296518]\n",
            "Train Epoch: 2 [8000/20000 (40%)]\tERM loss: 0.833289\tGrad penalty: 0.019094\n",
            "First 20 logits [-1.3859901 -1.6406479 -1.8854717  0.9189627 -1.8509718  1.5108215\n",
            "  1.2839006 -1.6872705 -1.5689718  1.4019139 -1.4326895  1.0711595\n",
            "  1.8696221 -1.8344706  1.3737898 -1.424454  -2.042937  -1.4747943\n",
            " -1.7219952 -1.4121377]\n",
            "Train Epoch: 2 [12000/20000 (60%)]\tERM loss: 0.822431\tGrad penalty: 0.020612\n",
            "First 20 logits [-1.4430026 -1.1717442 -1.4394838  1.8334204 -1.2674543  1.6172674\n",
            "  1.137522   0.8907265  1.6470926 -1.1211472 -1.5092208 -1.1977826\n",
            " -1.3282613 -1.6662489 -1.5441774  2.017538  -1.4525197  1.4684849\n",
            " -1.3693428  1.7769647]\n",
            "Train Epoch: 2 [16000/20000 (80%)]\tERM loss: 0.870354\tGrad penalty: 0.024771\n",
            "First 20 logits [-1.6306334 -1.6006129  2.2601743 -1.584161  -1.5809737  2.9498851\n",
            " -1.7813413  2.1140037 -1.4224776  2.261227   1.1915631  2.9103625\n",
            "  2.3341498 -1.8438175  2.0561142  2.6947918 -1.695985   2.5496721\n",
            "  2.6539881  2.1223714]\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4922, Accuracy: 15958/20000 (79.79%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.3312, Accuracy: 18152/20000 (90.76%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.5081, Accuracy: 1956/20000 (9.78%)\n",
            "\n",
            "Using penalty multiplier 10.0\n",
            "Train Epoch: 3 [0/20000 (0%)]\tERM loss: 0.809306\tGrad penalty: 0.014793\n",
            "First 20 logits [ 1.9651515  1.5161985 -1.5411184  1.9936476 -1.5114456 -1.4048159\n",
            " -1.5002899  2.403164   2.2572696 -1.5817869  1.4911274 -1.2924957\n",
            " -1.2956583  1.6909523  1.5680885  2.1078331  2.0034935 -1.450764\n",
            " -1.6520524 -1.5280901]\n",
            "Train Epoch: 3 [4000/20000 (20%)]\tERM loss: 0.812462\tGrad penalty: 0.015580\n",
            "First 20 logits [-1.9116474  2.203952  -1.6774925  1.0636193  1.3978332 -1.6872313\n",
            " -1.8246093  1.6930108 -1.5905576 -1.8549445 -1.639802   1.4835751\n",
            " -1.3002045 -1.2760626 -1.7836788  1.9631103 -1.7266767 -1.2733265\n",
            "  2.1551223 -1.747886 ]\n",
            "Train Epoch: 3 [8000/20000 (40%)]\tERM loss: 0.797562\tGrad penalty: 0.020796\n",
            "First 20 logits [-2.0338175  1.194575  -2.0973496  1.1494787  1.9903911  1.2207099\n",
            "  1.5201435  2.2076445  1.5711172 -1.4857339 -1.3573261 -2.1831613\n",
            "  2.0067608  1.4689118  1.9331981 -1.644928  -2.0366662 -2.336821\n",
            "  2.269374  -2.2881389]\n",
            "Train Epoch: 3 [12000/20000 (60%)]\tERM loss: 0.814514\tGrad penalty: 0.015561\n",
            "First 20 logits [-1.6543965  2.6245344 -1.8076895 -2.1634593  1.1781899 -1.6338602\n",
            "  1.2970719 -1.5156491 -1.8152875  2.5384932 -2.288915  -1.975363\n",
            "  1.638793   1.2693368  0.702932   2.062773   2.3379898  2.0750923\n",
            "  1.7889913 -2.0068312]\n",
            "Train Epoch: 3 [16000/20000 (80%)]\tERM loss: 0.815857\tGrad penalty: 0.013684\n",
            "First 20 logits [-1.6428593  -1.9747815  -2.1545882  -2.0711627   1.6933634  -2.0110643\n",
            " -2.0362747   0.73811466 -1.1533374  -1.5699432  -1.1929693  -1.0834401\n",
            "  0.5146182  -2.2173817  -2.1463897  -2.1241512   1.1478468   0.84373003\n",
            " -1.5521237  -1.7527303 ]\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4975, Accuracy: 15888/20000 (79.44%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.3207, Accuracy: 18012/20000 (90.06%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.6426, Accuracy: 2601/20000 (13.01%)\n",
            "\n",
            "Using penalty multiplier 20.0\n",
            "Train Epoch: 4 [0/20000 (0%)]\tERM loss: 0.829807\tGrad penalty: 0.014047\n",
            "First 20 logits [-2.1768382 -2.5905406  2.5925956 -2.8013084  1.4384937 -2.775345\n",
            " -2.1663744  2.2364404  1.8045464 -1.7241905 -2.5268161  2.8246403\n",
            " -2.5375063 -1.9606618  1.3582956 -2.138082   1.9314283 -2.3510284\n",
            " -2.0610306 -2.530763 ]\n",
            "Train Epoch: 4 [4000/20000 (20%)]\tERM loss: 0.805025\tGrad penalty: 0.017060\n",
            "First 20 logits [-2.0442517   1.7203741  -2.0239668  -0.21742226  1.3308425  -1.3417592\n",
            "  0.35645682 -0.0168462  -0.8384152  -1.3053828   0.59569323  0.9617709\n",
            "  1.2132957  -0.8764234   2.6183228  -1.960177   -2.57558    -1.9805795\n",
            " -1.5413496   0.7876636 ]\n",
            "Train Epoch: 4 [8000/20000 (40%)]\tERM loss: 0.845674\tGrad penalty: 0.006701\n",
            "First 20 logits [-1.8750427  -2.6553917  -2.4792333  -0.2646891  -2.499178   -1.9336572\n",
            "  1.979077    0.13608216 -1.1778752  -1.8255103   0.25952077 -1.0887495\n",
            " -2.0689585   0.6927378   1.7560486  -1.7219564  -2.113938    1.638066\n",
            " -1.9703002  -0.38173562]\n",
            "Train Epoch: 4 [12000/20000 (60%)]\tERM loss: 0.807437\tGrad penalty: 0.013399\n",
            "First 20 logits [-2.650123   -1.9698763   0.36806026 -2.091741   -0.10075561 -2.2984407\n",
            " -1.9568855  -1.9435409  -2.4245257   0.5536573   1.4488024  -1.5772208\n",
            " -2.751967   -2.5545251   3.1016197  -1.3771216   1.4856449   3.166588\n",
            " -2.3698986  -1.6879051 ]\n",
            "Train Epoch: 4 [16000/20000 (80%)]\tERM loss: 0.860430\tGrad penalty: 0.014174\n",
            "First 20 logits [-1.3776692  -1.8206162   0.24089631  0.33353683 -0.1986349  -0.4803347\n",
            "  1.8155296  -1.8236595   2.9324903  -1.8661867   1.4921075  -0.6496678\n",
            " -1.6463672   3.7874436  -0.16807069 -1.7156214  -0.27872017  2.2119298\n",
            "  1.8039145  -1.8026372 ]\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5263, Accuracy: 15090/20000 (75.45%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.4157, Accuracy: 16393/20000 (81.97%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.2448, Accuracy: 6062/20000 (30.31%)\n",
            "\n",
            "Using penalty multiplier 20.0\n",
            "Train Epoch: 5 [0/20000 (0%)]\tERM loss: 0.954037\tGrad penalty: 0.004858\n",
            "First 20 logits [ 0.98136157  0.8934227  -0.452119    3.2532146   4.042152    1.738368\n",
            "  3.2838771   0.09738129 -0.5379295  -1.3304248  -1.2454323   0.2787332\n",
            "  0.8048055   1.9671117   1.4318902   4.1442523   2.3706162   4.3301625\n",
            "  3.532941   -1.5078672 ]\n",
            "Train Epoch: 5 [4000/20000 (20%)]\tERM loss: 0.971703\tGrad penalty: 0.004263\n",
            "First 20 logits [-0.8901575   0.91252047  1.4264824  -0.4867389   0.35205805 -1.296434\n",
            "  2.0036244  -0.5524499   0.29961306 -0.7401312  -0.686336   -0.97500604\n",
            "  0.35110703  3.458461    1.1180999  -0.661221    1.107988    0.41228116\n",
            " -1.6090094  -1.8718457 ]\n",
            "Train Epoch: 5 [8000/20000 (40%)]\tERM loss: 0.962918\tGrad penalty: 0.004188\n",
            "First 20 logits [ 1.7142445  -0.01213553  0.7751006   3.5976      0.95512736 -1.3557043\n",
            "  3.2006438  -1.4478606  -0.4808133   3.084652   -0.5952308   0.43905884\n",
            "  3.1989365   0.8998902  -0.2913822   0.65963274 -0.71984386  2.2242494\n",
            " -1.0163865   1.8056569 ]\n",
            "Train Epoch: 5 [12000/20000 (60%)]\tERM loss: 0.942760\tGrad penalty: 0.001845\n",
            "First 20 logits [-0.46474293  1.7123759   1.3090823   3.501611    0.10608891  0.37103525\n",
            "  0.26380894  2.1769922   3.23281    -1.167161   -0.3122884   1.6241598\n",
            "  1.9410111   3.1344423   3.1420655   0.5940935  -1.4818071  -1.1788608\n",
            " -2.4427218   2.7143571 ]\n",
            "Train Epoch: 5 [16000/20000 (80%)]\tERM loss: 0.942595\tGrad penalty: 0.003883\n",
            "First 20 logits [-0.7220275   2.3839526  -0.25563946  1.7279973   2.230607    2.1132376\n",
            "  0.5487461   2.232614   -1.0556335   1.9285173  -0.4161641   0.35599473\n",
            "  4.600536    1.9889194  -1.1739506   0.70984566  2.383001   -0.32496107\n",
            " -0.20120643  2.6611464 ]\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4950, Accuracy: 15428/20000 (77.14%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.3951, Accuracy: 16711/20000 (83.56%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.2008, Accuracy: 6614/20000 (33.07%)\n",
            "\n",
            "Using penalty multiplier 30.0\n",
            "Train Epoch: 6 [0/20000 (0%)]\tERM loss: 0.902395\tGrad penalty: 0.003342\n",
            "First 20 logits [-2.5479474  -0.8753134   0.14789855  3.184596   -0.67353606 -1.1703717\n",
            " -0.09391952 -2.8193865  -0.7422487  -0.58564544  0.448678   -1.7976075\n",
            " -2.978247    1.9085815  -2.677041    2.3784037  -3.0541131  -2.5047998\n",
            " -0.7576641  -1.3992065 ]\n",
            "Train Epoch: 6 [4000/20000 (20%)]\tERM loss: 0.865202\tGrad penalty: 0.003722\n",
            "First 20 logits [-0.4863124   3.3893166  -2.2141871   2.3040743   1.253918    2.0184097\n",
            "  1.4040531   3.0365376  -1.2683583  -2.328622   -2.1093252  -1.5825704\n",
            "  0.7033383   1.5337894  -1.0284821   0.7314983  -1.2421224  -0.49190015\n",
            "  1.1744227  -0.909315  ]\n",
            "Train Epoch: 6 [8000/20000 (40%)]\tERM loss: 0.887352\tGrad penalty: 0.004863\n",
            "First 20 logits [ 1.0805025  -2.867772   -1.5934806  -0.25907934 -1.8484557   2.929564\n",
            " -1.5993835   1.1166358  -2.1667216   3.420567    0.37670666  1.5513299\n",
            " -2.6082861   2.093572    2.0730674  -1.5148857   1.2759084  -1.6005065\n",
            "  2.1181962  -3.6589928 ]\n",
            "Train Epoch: 6 [12000/20000 (60%)]\tERM loss: 0.848821\tGrad penalty: 0.002537\n",
            "First 20 logits [-1.8401921   0.23673563  0.7059771  -3.1011548  -2.3676472   1.1637971\n",
            " -0.30078244 -0.6887723  -1.991695    0.12010922  3.8046978  -0.20796356\n",
            " -2.0454726   0.6303157  -1.0034605  -1.3225484   3.0357332  -1.5817462\n",
            " -0.8195859  -2.944536  ]\n",
            "Train Epoch: 6 [16000/20000 (80%)]\tERM loss: 0.856951\tGrad penalty: 0.004056\n",
            "First 20 logits [ 0.00449883 -2.4892457   2.6325212   2.8166063  -0.93652326 -2.2327907\n",
            " -2.3553686  -2.4544995   2.3651676  -1.70682    -1.5483731  -0.6182781\n",
            " -1.9329636   0.6030736  -0.05890721  2.8368561  -2.13154    -1.1953248\n",
            " -0.8085523   1.38063   ]\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4773, Accuracy: 15426/20000 (77.13%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.3846, Accuracy: 16598/20000 (82.99%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.1449, Accuracy: 7176/20000 (35.88%)\n",
            "\n",
            "Using penalty multiplier 30.0\n",
            "Train Epoch: 7 [0/20000 (0%)]\tERM loss: 0.878612\tGrad penalty: 0.004729\n",
            "First 20 logits [ 0.09340152 -2.1323166  -0.77884287 -1.4191695   2.900927   -1.2894696\n",
            "  0.08914544  1.2134534  -0.8299862   3.3990881  -0.26108295  1.460386\n",
            " -0.0746939   0.5155528   0.26903298 -1.3803607  -2.066536    0.05724706\n",
            " -1.930336   -0.38517803]\n",
            "Train Epoch: 7 [4000/20000 (20%)]\tERM loss: 0.916615\tGrad penalty: 0.001432\n",
            "First 20 logits [ 0.99718815  1.6247112  -3.0718303  -0.07917852  0.5786882  -2.150772\n",
            "  2.6549573  -0.22106321 -1.3725946  -1.541203   -1.5109781  -1.3452759\n",
            "  3.9201412  -0.42734897 -1.464051   -1.0935519  -0.8406392  -1.5438676\n",
            "  1.1925321  -1.8145565 ]\n",
            "Train Epoch: 7 [8000/20000 (40%)]\tERM loss: 0.901047\tGrad penalty: 0.000805\n",
            "First 20 logits [-0.44946444  2.5664234   0.17736514  0.68693936  0.62601924  2.22841\n",
            "  0.1763643   0.2916352   2.980629   -1.213134    1.136205   -2.7274396\n",
            "  0.09077062  1.7566997  -1.7958239  -0.33791125 -2.416632    0.9927792\n",
            " -0.14337118 -0.6455463 ]\n",
            "Train Epoch: 7 [12000/20000 (60%)]\tERM loss: 0.875918\tGrad penalty: 0.001035\n",
            "First 20 logits [ 1.4692032   2.2866373   2.0304253  -2.5606267  -2.287386    0.15191776\n",
            " -3.0969687  -2.9285982   2.9516852  -1.3670877  -0.41659108 -0.30987644\n",
            " -0.03385605 -1.6598238  -1.5909579   0.50108606 -1.7419381   2.365858\n",
            "  1.685798   -2.7652125 ]\n",
            "Train Epoch: 7 [16000/20000 (80%)]\tERM loss: 0.835568\tGrad penalty: 0.003251\n",
            "First 20 logits [ 1.9180038   0.15315475  0.17360276  2.4431672  -0.0979234   2.8372908\n",
            " -0.10066443  1.41        1.4381154   2.1901197  -2.8509462  -0.2849915\n",
            "  2.0752788  -2.4599338  -0.25929585 -2.1801581   2.3727484  -1.1988088\n",
            "  0.05606681  2.915232  ]\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4691, Accuracy: 15669/20000 (78.34%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.3504, Accuracy: 16938/20000 (84.69%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.3330, Accuracy: 6284/20000 (31.42%)\n",
            "\n",
            "Using penalty multiplier 40.0\n",
            "Train Epoch: 8 [0/20000 (0%)]\tERM loss: 0.814519\tGrad penalty: 0.002659\n",
            "First 20 logits [-0.22809947  2.9509676   3.7142966  -1.4382259  -0.64763576  0.7124127\n",
            "  0.00423656 -1.2649144   3.095117    0.24902087  2.0637019   0.2279646\n",
            "  1.8389041   3.1959758   3.324354   -1.8587178   1.4061803  -1.1015981\n",
            "  1.8657582  -2.7609181 ]\n",
            "Train Epoch: 8 [4000/20000 (20%)]\tERM loss: 0.847574\tGrad penalty: 0.000597\n",
            "First 20 logits [ 0.23506078  1.2680179  -2.9265099   0.93743896  0.47069478 -2.1326258\n",
            " -3.2410328   2.5496497  -0.62013036  2.658413   -0.79864967  2.0220895\n",
            " -2.2274854   0.48376164  0.81056994  1.8798124   2.1577806   1.8605427\n",
            "  0.5050912  -1.4632013 ]\n",
            "Train Epoch: 8 [8000/20000 (40%)]\tERM loss: 0.921791\tGrad penalty: 0.001531\n",
            "First 20 logits [ 0.5714621   1.833552   -2.1795027  -0.768774   -1.0782105   2.266558\n",
            " -1.338642   -3.0196106  -2.0961902  -2.7919655   2.4887004  -0.52366805\n",
            " -1.8901197   2.171062    3.0186625  -1.87337    -2.1992376   2.1264417\n",
            " -1.8998784  -2.8789046 ]\n",
            "Train Epoch: 8 [12000/20000 (60%)]\tERM loss: 0.960489\tGrad penalty: 0.002514\n",
            "First 20 logits [ 0.97239494  2.1604264  -0.8931966   0.40808928 -0.10887922  1.6387497\n",
            " -1.0015985  -2.1948614  -2.532572    0.67482567 -0.37236878  0.88566965\n",
            " -1.4257405  -0.30430317  1.7535146  -1.9762541  -0.50332814  1.5869752\n",
            " -1.873276   -4.0356655 ]\n",
            "Train Epoch: 8 [16000/20000 (80%)]\tERM loss: 0.939441\tGrad penalty: -0.000142\n",
            "First 20 logits [-0.85377103  2.869535   -0.6263157   0.77461237  1.2821194   3.0209835\n",
            "  2.1552734  -1.369903    2.3784075   2.7282438  -1.4713819   0.11827395\n",
            "  1.1178801   3.1200113  -1.0415745   0.316062    3.3409972   2.100346\n",
            " -0.82328904  0.48426843]\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4773, Accuracy: 15322/20000 (76.61%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.4137, Accuracy: 15845/20000 (79.22%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.9941, Accuracy: 9835/20000 (49.17%)\n",
            "\n",
            "Using penalty multiplier 40.0\n",
            "Train Epoch: 9 [0/20000 (0%)]\tERM loss: 0.887844\tGrad penalty: 0.002362\n",
            "First 20 logits [ 0.45157552 -1.241702   -0.7351471  -1.3732355  -0.94726074 -0.3329343\n",
            " -1.6953381   2.5762334   1.2300047  -0.18981585  0.3668216   0.00463119\n",
            "  0.07039683  0.99298704 -0.2235141  -0.7649138  -1.721776   -1.3205833\n",
            " -0.39668798 -2.2737753 ]\n",
            "Train Epoch: 9 [4000/20000 (20%)]\tERM loss: 0.901338\tGrad penalty: 0.004611\n",
            "First 20 logits [ 1.7189331  -2.8243928   1.2709213   2.336495    2.0926633   0.03433338\n",
            " -2.3613877   2.2813568  -3.4679928   2.0819232  -1.434506   -0.8088033\n",
            " -2.8937302  -0.08450627 -0.50401205 -3.3185139  -1.0174762  -1.6367532\n",
            " -3.1189163  -2.1925228 ]\n",
            "Train Epoch: 9 [8000/20000 (40%)]\tERM loss: 0.864492\tGrad penalty: 0.003210\n",
            "First 20 logits [ 0.78406274 -3.1431973  -2.4107602   2.1208334  -2.4963078   2.1119363\n",
            "  0.8540713  -0.32372665 -0.5162461   0.10600673  1.6933457   2.6106112\n",
            " -2.341716    2.487821    2.2331238   1.4147882  -1.705453   -1.6414462\n",
            "  1.4005384  -1.6249496 ]\n",
            "Train Epoch: 9 [12000/20000 (60%)]\tERM loss: 0.807610\tGrad penalty: 0.003098\n",
            "First 20 logits [-2.9523618  -2.5887833  -0.3494092  -0.32467642 -1.3418914  -1.3074901\n",
            "  3.2545774  -0.33520532  2.1884136   1.539427    0.30287015 -2.0666542\n",
            " -2.2988272   1.8415459  -2.4581854   3.4013386   2.9164736  -1.4696207\n",
            " -2.6314673   0.20751856]\n",
            "Train Epoch: 9 [16000/20000 (80%)]\tERM loss: 0.880844\tGrad penalty: 0.004936\n",
            "First 20 logits [ 1.4472620e+00  1.9615023e+00  2.8374078e+00  2.5948858e+00\n",
            "  4.0669980e+00 -9.5793641e-01  3.2916927e+00  1.2274383e+00\n",
            " -3.7170994e-01  6.9279736e-01  3.8822196e+00  2.7455506e+00\n",
            " -1.6199201e+00  2.1515433e-01 -8.7384903e-01  2.4601059e-01\n",
            " -1.9756086e+00 -9.4127774e-01 -2.8360812e-03 -1.6282799e+00]\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4731, Accuracy: 15476/20000 (77.38%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.4042, Accuracy: 16216/20000 (81.08%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.0369, Accuracy: 8807/20000 (44.03%)\n",
            "\n",
            "Using penalty multiplier 50.0\n",
            "Train Epoch: 10 [0/20000 (0%)]\tERM loss: 0.866424\tGrad penalty: 0.003039\n",
            "First 20 logits [-2.4453125  -2.4321198   0.05752476 -2.1929653  -0.7735479   2.2624993\n",
            "  2.2275066   0.8084568  -1.9541355   2.1362367  -2.1293857  -0.66949725\n",
            " -0.2890685  -0.2849395   1.5169754  -2.0823016  -1.5005788   1.5527713\n",
            " -0.20107785 -1.0092849 ]\n",
            "Train Epoch: 10 [4000/20000 (20%)]\tERM loss: 0.944483\tGrad penalty: 0.001144\n",
            "First 20 logits [ 1.6731209  -0.35648638  0.1256203   1.3656896   1.2417867  -3.1482675\n",
            " -0.2219467   1.5275296   1.7090516  -2.9077828  -2.9092207   2.5432363\n",
            " -0.7962328   1.6876287  -1.0892338   2.0728629  -1.7535005  -2.5203705\n",
            " -2.0637095  -0.06012416]\n",
            "Train Epoch: 10 [8000/20000 (40%)]\tERM loss: 0.985445\tGrad penalty: 0.001692\n",
            "First 20 logits [ 0.6493539  -1.2412895   1.7511004  -1.5900937   1.6674428   0.4192341\n",
            "  2.384796    0.20477432 -0.2896641   1.7378882  -1.8922368  -2.5407372\n",
            " -0.06594701 -2.7014756  -0.8254076  -2.6988156  -1.8093872   1.990004\n",
            " -2.0476081  -1.7646127 ]\n",
            "Train Epoch: 10 [12000/20000 (60%)]\tERM loss: 0.957925\tGrad penalty: 0.000762\n",
            "First 20 logits [ 0.21595843 -1.5403272  -2.4017859   0.45625457 -1.1460478   0.9510732\n",
            " -0.44047353 -2.3744118  -0.97184885 -0.46622023  1.9381957   0.12416502\n",
            "  1.3987253  -1.9080815  -1.9279219  -1.6569439   1.0734328   2.1846683\n",
            "  0.21813013  1.2028433 ]\n",
            "Train Epoch: 10 [16000/20000 (80%)]\tERM loss: 0.959804\tGrad penalty: 0.002282\n",
            "First 20 logits [-2.526944    0.58374715 -0.42421788  2.3399     -2.4087987   1.0671914\n",
            " -2.435546    2.5374804   0.61868423  0.1894878  -2.1673493  -0.4600206\n",
            " -2.9325051  -2.3584485   1.2120185   0.8589849  -2.5285573  -3.497419\n",
            " -1.3216636  -1.9137819 ]\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4802, Accuracy: 15305/20000 (76.53%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.4247, Accuracy: 15799/20000 (79.00%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.9708, Accuracy: 10247/20000 (51.23%)\n",
            "\n",
            "Using penalty multiplier 50.0\n",
            "Train Epoch: 11 [0/20000 (0%)]\tERM loss: 0.910107\tGrad penalty: 0.000589\n",
            "First 20 logits [-1.6312917  -3.1453147  -1.3995377  -3.3050609   0.5435086  -1.0452\n",
            " -2.8993287  -2.8469164  -2.4011643  -2.7227347  -2.2994432  -0.46561143\n",
            " -2.9235048  -2.8147624  -0.9751377   2.0717902   2.3292778   2.5226917\n",
            " -3.2035365  -1.4181874 ]\n",
            "Train Epoch: 11 [4000/20000 (20%)]\tERM loss: 0.872308\tGrad penalty: 0.000621\n",
            "First 20 logits [-2.9208899  -0.8998057  -1.7889178  -0.24560662 -0.726439    3.334768\n",
            "  0.93736583 -1.6787746   0.96284145  2.7846959  -0.15100847  3.7327883\n",
            "  2.2344713  -2.2878382  -1.532069    2.8654068  -2.5301816   0.5219121\n",
            "  0.19874649  2.1737666 ]\n",
            "Train Epoch: 11 [8000/20000 (40%)]\tERM loss: 0.877222\tGrad penalty: 0.001093\n",
            "First 20 logits [-0.26277003  1.0419847   1.2345088   2.3009193  -2.9241896  -1.3881806\n",
            " -1.1718395  -2.3752153   2.7896698   3.2454414  -0.85391057  0.39807564\n",
            "  2.4484916  -0.30334768  0.23500744  3.1347458  -2.655188   -2.4293172\n",
            " -1.2247561   0.8979867 ]\n",
            "Train Epoch: 11 [12000/20000 (60%)]\tERM loss: 0.830443\tGrad penalty: 0.003554\n",
            "First 20 logits [ 2.4406707   2.447957   -2.7287948  -1.5330538   1.7855088   0.8973955\n",
            "  0.02444653  0.6154761   1.0177279   2.0462427   2.0349371  -0.33547723\n",
            " -1.4055053   2.2141156   1.2780554  -2.275772   -0.30015352 -0.5711019\n",
            " -1.5023893   1.8365259 ]\n",
            "Train Epoch: 11 [16000/20000 (80%)]\tERM loss: 0.831143\tGrad penalty: 0.002546\n",
            "First 20 logits [-0.18928277  2.1977956  -0.20265989 -2.8524306  -0.7165456  -3.3433259\n",
            " -3.2022512   2.3889208   1.657998   -2.979836    1.6731635   2.5128872\n",
            "  1.9261214  -0.78329086  1.6534863   2.225757    2.2054033  -3.648294\n",
            "  1.495349   -0.67158765]\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4569, Accuracy: 15586/20000 (77.93%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.3751, Accuracy: 16393/20000 (81.97%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.1192, Accuracy: 8499/20000 (42.49%)\n",
            "\n",
            "Using penalty multiplier 60.0\n",
            "Train Epoch: 12 [0/20000 (0%)]\tERM loss: 0.842757\tGrad penalty: 0.002406\n",
            "First 20 logits [-1.1263933  -1.232252    0.85726064 -2.6156442   0.2954394   0.975895\n",
            "  0.18192457 -3.5128398  -0.55259824  0.8577857   0.56740946  3.909005\n",
            " -1.4662005  -2.1996713   0.99021584 -2.6339824  -1.6922826  -2.0371253\n",
            "  4.142809    2.1368923 ]\n",
            "Train Epoch: 12 [4000/20000 (20%)]\tERM loss: 0.906278\tGrad penalty: 0.001415\n",
            "First 20 logits [-1.733165    0.24270329 -0.70356894 -0.13357167 -2.5568483  -2.588833\n",
            "  2.4437523  -0.4323496   0.0133021   0.52916056  0.9930327   0.90248895\n",
            " -0.45524508  2.5012805  -1.9818041   1.9790262   0.02372837  1.6701276\n",
            " -2.3038375  -0.46486908]\n",
            "Train Epoch: 12 [8000/20000 (40%)]\tERM loss: 0.932552\tGrad penalty: 0.000093\n",
            "First 20 logits [ 1.1432356  -1.3822454  -0.5726158  -1.6024452  -3.0680237  -2.9712093\n",
            " -0.60552865  2.1127348  -1.7806938   1.4239063  -0.20415789 -0.99808395\n",
            " -1.7015874  -1.3584     -0.10934236 -0.23074543  0.38977027  1.5178235\n",
            "  2.5037901   3.4255009 ]\n",
            "Train Epoch: 12 [12000/20000 (60%)]\tERM loss: 0.942324\tGrad penalty: 0.000530\n",
            "First 20 logits [-2.0549397 -1.0280124 -3.244171   2.6922543  0.2903147  3.414869\n",
            "  0.2001457 -1.9526227 -0.4720509  1.9630966  1.0836957  2.7722712\n",
            "  2.6918933  3.0414884  1.4473569 -1.5234501  3.2660902  0.3579979\n",
            "  3.4201612 -1.0658076]\n",
            "Train Epoch: 12 [16000/20000 (80%)]\tERM loss: 0.918732\tGrad penalty: 0.000476\n",
            "First 20 logits [-3.049916    0.92017204 -2.187085   -1.1250519  -0.47145984  1.3104423\n",
            " -2.3607588   2.6425705   2.108846    1.8344545   2.6421187   1.6273535\n",
            " -1.7468387  -1.8937488  -0.35100833  0.80626965 -1.0921845   2.1895049\n",
            "  0.64294565  2.1899025 ]\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4698, Accuracy: 15395/20000 (76.97%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.4115, Accuracy: 15913/20000 (79.56%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.9993, Accuracy: 10166/20000 (50.83%)\n",
            "\n",
            "Using penalty multiplier 60.0\n",
            "Train Epoch: 13 [0/20000 (0%)]\tERM loss: 0.895601\tGrad penalty: 0.000499\n",
            "First 20 logits [-1.115425    0.41347685 -3.7684453  -0.33917016 -1.2340006   0.06481159\n",
            "  0.11087106 -2.1268356  -0.48299727  0.34176582 -2.0445564  -1.3179883\n",
            " -2.08827     1.6042148   0.6949436  -2.4161503   1.2882016  -1.742898\n",
            "  1.9126507  -1.6090732 ]\n",
            "Train Epoch: 13 [4000/20000 (20%)]\tERM loss: 0.863242\tGrad penalty: 0.000151\n",
            "First 20 logits [-3.2761812   3.0021937   0.3213282  -1.1800297  -1.0587136  -2.65976\n",
            " -2.6277242  -1.5305839   2.3906794  -2.3654158   2.006464   -2.2508762\n",
            "  2.63668    -3.0790167  -2.818963    1.1894462  -0.80606973 -2.8109477\n",
            " -2.7772381   3.3441207 ]\n",
            "Train Epoch: 13 [8000/20000 (40%)]\tERM loss: 0.846909\tGrad penalty: 0.001732\n",
            "First 20 logits [ 3.3198657  -0.960589    3.5655992   3.5053718  -3.2132723   2.3179793\n",
            "  2.8486938  -1.8375502   2.526995    1.1093829  -1.4227006  -1.3482152\n",
            " -2.091959   -0.61191183 -2.7080793  -0.99999785 -3.2669187   0.5484385\n",
            " -1.1173625  -1.3584602 ]\n",
            "Train Epoch: 13 [12000/20000 (60%)]\tERM loss: 0.811186\tGrad penalty: 0.002088\n",
            "First 20 logits [ 2.4530823   1.5404193  -0.53918403  2.3283498  -2.2232602  -1.8038142\n",
            " -1.7905715   2.6033967  -2.1618102  -3.0478175   0.17749965  0.54472274\n",
            " -0.11863808 -2.1620185  -3.0051086   0.7147719  -2.6775339  -0.19005327\n",
            "  3.3284192  -0.28095955]\n",
            "Train Epoch: 13 [16000/20000 (80%)]\tERM loss: 0.862305\tGrad penalty: 0.002306\n",
            "First 20 logits [-1.2802256  -1.4298856  -0.01718808  0.26311222 -2.0016856   2.7761087\n",
            "  2.2526586   0.67418885 -2.9823527   3.14177    -3.905719   -0.5852538\n",
            "  3.8101048  -0.81751585 -2.8972104   0.47280705  2.1552665  -2.7292743\n",
            " -2.7428694   0.49400485]\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4636, Accuracy: 15444/20000 (77.22%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.4059, Accuracy: 15867/20000 (79.33%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.9896, Accuracy: 10267/20000 (51.34%)\n",
            "\n",
            "Using penalty multiplier 70.0\n",
            "Train Epoch: 14 [0/20000 (0%)]\tERM loss: 0.896080\tGrad penalty: 0.001702\n",
            "First 20 logits [ 1.7491102  -0.583798    3.513298   -1.531015   -0.8822025   0.89854175\n",
            "  2.0991387  -0.8419977   0.10008449 -1.4510497   1.6849667  -2.5138283\n",
            " -1.1318605  -2.2398756   3.8511875   0.39529678 -1.2560501  -0.2851882\n",
            "  2.7124934  -1.8827684 ]\n",
            "Train Epoch: 14 [4000/20000 (20%)]\tERM loss: 0.903772\tGrad penalty: 0.000310\n",
            "First 20 logits [ 1.3377520e-01  2.5613973e+00 -1.1175014e+00 -6.4034361e-01\n",
            " -7.6501048e-04  7.3028338e-01  2.0175076e+00  1.6095411e+00\n",
            " -1.0056314e-01  2.1363113e+00 -2.5502098e+00  2.6262646e+00\n",
            "  9.8077118e-01 -2.4620993e+00  2.0565042e+00  2.6440194e+00\n",
            " -1.0549886e+00 -2.5506895e+00  5.1194054e-01  1.6358176e+00]\n",
            "Train Epoch: 14 [8000/20000 (40%)]\tERM loss: 0.904380\tGrad penalty: 0.000563\n",
            "First 20 logits [-3.1528697  -1.0117792  -3.0903676   0.16900355 -3.1951942  -2.3077717\n",
            " -1.962455    1.0009047   2.8063993   3.0840518  -1.3378967  -3.4940314\n",
            "  0.09228712 -3.8173964   0.46058193  2.5629945  -1.258767    2.2761238\n",
            "  2.0991182  -2.249276  ]\n",
            "Train Epoch: 14 [12000/20000 (60%)]\tERM loss: 0.863479\tGrad penalty: 0.001800\n",
            "First 20 logits [-1.8550215   1.0780787  -0.00418643 -0.5338754  -1.2872429  -2.7128575\n",
            " -0.9147724  -1.7351388  -0.68418634  2.3300955  -0.5851946   3.1278894\n",
            " -2.3031774  -2.8874097  -0.43306583 -1.2782636  -0.799755    0.6250895\n",
            " -2.101217   -0.4541232 ]\n",
            "Train Epoch: 14 [16000/20000 (80%)]\tERM loss: 0.847973\tGrad penalty: 0.003944\n",
            "First 20 logits [ 1.8461883   2.8259392  -2.3902383   3.33651    -2.1754177   1.2742397\n",
            " -0.55220246  0.8525608   1.4496223  -2.3430204  -0.0409813  -3.0498195\n",
            " -1.5562319  -1.2201785   1.3483136  -1.9211633  -2.1295676  -0.26128855\n",
            " -2.6531823   1.214294  ]\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4621, Accuracy: 15467/20000 (77.33%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.3958, Accuracy: 15967/20000 (79.83%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.0742, Accuracy: 9885/20000 (49.42%)\n",
            "\n",
            "Using penalty multiplier 70.0\n",
            "Train Epoch: 15 [0/20000 (0%)]\tERM loss: 0.846831\tGrad penalty: 0.002062\n",
            "First 20 logits [-2.4639678  -1.3600836  -2.5266762  -0.9285724   3.4004545  -1.7994369\n",
            "  2.5613651  -2.5509772  -3.7587504   0.6137261  -1.6204668   0.25448084\n",
            "  3.6004076   1.5280594   1.4608154   1.4814395  -1.8130704   3.2590618\n",
            " -1.3644004  -3.1272502 ]\n",
            "Train Epoch: 15 [4000/20000 (20%)]\tERM loss: 0.857346\tGrad penalty: 0.000403\n",
            "First 20 logits [ 2.7729683   0.73388076  1.0345932  -1.2490417  -0.17352162 -2.2545316\n",
            " -1.456921    2.2064097   0.90504384  1.6317286   0.32078868  0.8827435\n",
            "  0.86331725 -0.03270933 -3.2587326  -1.1725409   2.4261825   2.0616677\n",
            " -1.3632107  -2.6691628 ]\n",
            "Train Epoch: 15 [8000/20000 (40%)]\tERM loss: 0.881952\tGrad penalty: 0.001291\n",
            "First 20 logits [-0.54531324 -1.0352496  -2.683915   -2.5721545   0.99050283 -1.5928723\n",
            " -1.8316351   0.5262915  -3.2240903  -0.7059136  -0.72484046  2.0191686\n",
            " -1.5065359  -2.8547301   3.3758192   3.9148943  -1.8711802   1.4799005\n",
            "  3.20688    -2.7939842 ]\n",
            "Train Epoch: 15 [12000/20000 (60%)]\tERM loss: 0.877814\tGrad penalty: 0.000813\n",
            "First 20 logits [ 2.1661823  -2.7897742  -1.5567567  -3.0817032  -1.4002092   0.80478424\n",
            " -0.60983473  0.92905545 -2.8423193  -1.4775115  -3.2033315   0.82017773\n",
            " -1.219708   -1.0523392  -0.38927424  1.7966416   2.4175472   2.0200758\n",
            " -3.1163194  -2.9555287 ]\n",
            "Train Epoch: 15 [16000/20000 (80%)]\tERM loss: 0.884360\tGrad penalty: 0.000038\n",
            "First 20 logits [-2.2770941  -0.58004963  1.3416525  -1.7628512  -0.34745455 -1.8333676\n",
            "  0.12782207  1.9765539   1.853363   -2.2092545   1.905038    0.55947036\n",
            " -2.5141757   2.2514775   0.09903552 -3.2970376  -2.973851   -2.3126247\n",
            "  3.2711596   3.1066206 ]\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4630, Accuracy: 15467/20000 (77.33%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.4087, Accuracy: 15833/20000 (79.17%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.9965, Accuracy: 10319/20000 (51.59%)\n",
            "\n",
            "Using penalty multiplier 80.0\n",
            "Train Epoch: 16 [0/20000 (0%)]\tERM loss: 0.863371\tGrad penalty: 0.001291\n",
            "First 20 logits [ 1.1373343  -2.7255895   1.3268831   2.2618158   0.24229178 -0.83225346\n",
            " -1.8150469  -2.399787    0.34920812 -2.7300594   1.8571689   0.7870059\n",
            "  3.1494236   0.18266493  2.661293    0.12613371 -3.023809   -2.101899\n",
            " -2.6050045   0.28824973]\n",
            "Train Epoch: 16 [4000/20000 (20%)]\tERM loss: 0.855063\tGrad penalty: 0.000549\n",
            "First 20 logits [-1.1739371   3.7934477   0.8611303   1.6651601   1.9046911  -0.4307803\n",
            " -0.15501247  1.8718624  -0.8199333   1.7077669  -0.12328459  0.39387578\n",
            " -1.6753241  -0.2030662   2.242168   -2.925753   -3.2420905  -3.2685397\n",
            "  0.28678495 -0.62004924]\n",
            "Train Epoch: 16 [8000/20000 (40%)]\tERM loss: 0.829635\tGrad penalty: 0.000923\n",
            "First 20 logits [-0.48844114  0.19089894 -0.72016484 -1.9780649   0.27911672 -2.2570667\n",
            " -3.4392016  -2.8221743  -2.814744   -1.2169347  -0.14505206 -1.7855315\n",
            "  0.06039442 -3.0627582  -1.6185367  -0.066689   -0.06721207 -1.278361\n",
            " -0.95884424  1.3874762 ]\n",
            "Train Epoch: 16 [12000/20000 (60%)]\tERM loss: 0.886887\tGrad penalty: 0.001257\n",
            "First 20 logits [ 3.8681955   2.2054534  -1.7377051  -0.86949235  2.894483   -3.363525\n",
            "  3.3762736  -1.1781099   0.34910527  1.2837362  -3.9719691   2.5813909\n",
            "  1.9389455   1.450143    0.77108014  1.3617368   1.7840005   0.94276744\n",
            " -0.93031627 -2.0948982 ]\n",
            "Train Epoch: 16 [16000/20000 (80%)]\tERM loss: 1.001337\tGrad penalty: 0.000600\n",
            "First 20 logits [ 0.57103217  3.007859    2.7183049  -0.27122235  1.6082718   0.06137787\n",
            "  0.91064143 -1.0044441   1.3021673   3.5897005   3.0553348   0.2665396\n",
            " -0.79230875 -1.4597118  -0.750111    1.7974268   2.7046776   3.4941225\n",
            " -0.5456795   1.3522606 ]\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5025, Accuracy: 15053/20000 (75.27%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.4694, Accuracy: 15300/20000 (76.50%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.8803, Accuracy: 11138/20000 (55.69%)\n",
            "\n",
            "Using penalty multiplier 80.0\n",
            "Train Epoch: 17 [0/20000 (0%)]\tERM loss: 0.961392\tGrad penalty: 0.000534\n",
            "First 20 logits [ 3.598873    0.66466165  0.808697   -0.7289811   0.44456065 -0.15096617\n",
            "  3.0829504  -1.3491229   1.6834927   1.5664382  -1.4533215   2.0904882\n",
            "  0.9232999   1.7170296   0.05633018  0.8647812  -2.0669835   2.809815\n",
            "  1.1278015  -1.1756252 ]\n",
            "Train Epoch: 17 [4000/20000 (20%)]\tERM loss: 0.941131\tGrad penalty: 0.000248\n",
            "First 20 logits [-1.5715665   3.1474698   0.95043665  2.3379056  -2.1858778   0.8452347\n",
            "  1.7296091   0.59618986  2.4013858   2.343037    0.09484053  0.43683347\n",
            " -0.40565476 -2.4645584  -0.00714719 -0.817947   -0.4373879   2.8452537\n",
            " -0.5856457  -1.6162523 ]\n",
            "Train Epoch: 17 [8000/20000 (40%)]\tERM loss: 0.918990\tGrad penalty: -0.000329\n",
            "First 20 logits [-1.1140661   0.26992735  0.19517538  0.6987418   1.4551356  -0.8991243\n",
            "  0.16288084 -2.0925524   2.1843019   0.17421807  0.65307826  0.24266298\n",
            "  1.7417028   1.1748545  -1.1339718   1.2551118   3.2062972  -0.43452206\n",
            "  0.26074564 -2.2626216 ]\n",
            "Train Epoch: 17 [12000/20000 (60%)]\tERM loss: 0.857816\tGrad penalty: 0.001024\n",
            "First 20 logits [-2.8714087  -1.7861679   3.0490994   2.8800924   2.508924   -1.2164514\n",
            "  2.5128174   1.5427572   3.240261   -0.22930762  0.02795321  0.52457064\n",
            "  2.4457166   1.9460638  -1.8256739  -2.5923846  -0.75183946 -1.653457\n",
            " -0.6631795   1.7291315 ]\n",
            "Train Epoch: 17 [16000/20000 (80%)]\tERM loss: 0.881825\tGrad penalty: 0.002564\n",
            "First 20 logits [-1.9897145  -0.39414746 -1.3393173  -2.5883112   0.37664077 -2.280862\n",
            " -0.99020374  2.4782157  -1.3163831   1.7453352  -2.950051   -0.27492675\n",
            "  0.13240193 -0.5962522   2.713444   -3.774968   -2.0184608   1.2279972\n",
            "  2.9785476  -3.4704435 ]\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4627, Accuracy: 15509/20000 (77.55%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.4159, Accuracy: 15795/20000 (78.97%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.9424, Accuracy: 10616/20000 (53.08%)\n",
            "\n",
            "Using penalty multiplier 90.0\n",
            "Train Epoch: 18 [0/20000 (0%)]\tERM loss: 0.878882\tGrad penalty: 0.003791\n",
            "First 20 logits [-0.14382456 -0.44057283 -2.727122    0.3391097   3.330238   -1.411239\n",
            "  2.6398234  -1.0295558  -0.6170569  -0.46920288  2.363256   -0.9535496\n",
            " -2.0574858   0.5694491  -2.2496598  -0.61028147  2.0598679  -0.7937494\n",
            " -0.1187004   1.8032348 ]\n",
            "Train Epoch: 18 [4000/20000 (20%)]\tERM loss: 0.923984\tGrad penalty: 0.000050\n",
            "First 20 logits [-0.572495    1.9268618  -2.9067168   2.1411767  -0.7863957  -0.21550696\n",
            "  3.4238815   1.9008075  -2.858538   -0.5799522  -1.5159761   2.937373\n",
            "  0.27792454 -0.69025975  2.5643365   0.2624739   1.9876356  -1.0448936\n",
            "  0.7236922  -1.5627801 ]\n",
            "Train Epoch: 18 [8000/20000 (40%)]\tERM loss: 0.897600\tGrad penalty: 0.000270\n",
            "First 20 logits [ 1.8045969   0.99747837  2.007921    2.9661841  -2.4971166  -2.816199\n",
            "  3.0152743  -0.5533824  -1.7293087   3.2006528   1.0492454  -2.451613\n",
            "  2.3842473   2.4746563   1.3104302  -2.8470166  -1.4683094  -0.34200943\n",
            "  0.9335743  -0.3111715 ]\n",
            "Train Epoch: 18 [12000/20000 (60%)]\tERM loss: 0.874185\tGrad penalty: 0.000843\n",
            "First 20 logits [ 0.2296856   1.92908    -0.8029411  -0.21009368  1.5206995  -2.2258663\n",
            " -1.6084023   0.720902    1.4178998  -0.30695704 -1.4373419   1.0784564\n",
            "  2.2921062  -2.2728713   0.31115094 -0.36706612 -1.264103   -1.7973269\n",
            " -1.468493    1.9870064 ]\n",
            "Train Epoch: 18 [16000/20000 (80%)]\tERM loss: 0.855034\tGrad penalty: 0.002704\n",
            "First 20 logits [ 2.0739684   3.7349777   3.5296438  -2.467583   -3.55674     3.485851\n",
            " -1.8706632   1.0564     -1.5944713   3.6855574   0.5485027  -2.6870592\n",
            "  3.4911535   2.131726    3.036705    0.9400089  -1.6562734  -3.7559223\n",
            "  0.30622673 -0.08464988]\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4521, Accuracy: 15665/20000 (78.33%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.3715, Accuracy: 16479/20000 (82.39%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.1569, Accuracy: 7979/20000 (39.90%)\n",
            "\n",
            "Using penalty multiplier 90.0\n",
            "Train Epoch: 19 [0/20000 (0%)]\tERM loss: 0.818429\tGrad penalty: 0.003246\n",
            "First 20 logits [ 1.163797    3.602458    0.75269294 -3.3853717   2.0941775   2.1797192\n",
            " -0.42467868 -1.7247638  -2.1644535  -0.26521876  1.6820884   2.9818215\n",
            "  3.8016598  -0.801091    2.3884969   0.8109625   3.435096   -0.4337617\n",
            "  3.1372411   2.2951133 ]\n",
            "Train Epoch: 19 [4000/20000 (20%)]\tERM loss: 0.829518\tGrad penalty: 0.001271\n",
            "First 20 logits [ 1.9424585  -2.3654475   1.6956948  -0.43900153 -2.9554398   3.076835\n",
            "  1.3474678   0.3742691   3.5685325   2.7678392  -2.571959   -2.880107\n",
            " -2.01569    -3.006391    3.0894868  -0.8794407   3.4473796   2.8472998\n",
            " -1.4987986   0.85573924]\n",
            "Train Epoch: 19 [8000/20000 (40%)]\tERM loss: 0.843250\tGrad penalty: -0.000075\n",
            "First 20 logits [-2.1809552  -0.15859686  1.4254003  -1.9517759  -2.2236197  -2.5227854\n",
            " -0.06877063 -2.990772   -0.7782096   0.68637496  0.3473614  -0.6448459\n",
            " -1.9145998  -2.9388294   0.5326495  -1.7705256   0.8633749   0.10753182\n",
            "  3.5197375   1.5307126 ]\n",
            "Train Epoch: 19 [12000/20000 (60%)]\tERM loss: 0.891694\tGrad penalty: -0.000285\n",
            "First 20 logits [ 0.94859856  0.13270608  0.7750268   0.7596351  -0.01887932 -3.357285\n",
            " -1.8027616  -0.59761983 -0.06063808  1.9037309  -2.031934    0.95683604\n",
            " -0.23714061  1.3464365   2.3969605   1.7653117   3.2334738  -3.835954\n",
            "  0.15726306  0.42400795]\n",
            "Train Epoch: 19 [16000/20000 (80%)]\tERM loss: 0.907607\tGrad penalty: -0.000071\n",
            "First 20 logits [-3.3046966   1.0964772  -3.5916839  -3.8760285   0.17560278 -2.4106884\n",
            "  0.87844014  2.4808104  -1.3750352   1.4497339  -0.29181483  1.5271287\n",
            " -2.828996    0.10235926  1.6051618  -1.2512273   0.53824234 -0.87079096\n",
            " -2.6591895   2.2314777 ]\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4811, Accuracy: 15308/20000 (76.54%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.4402, Accuracy: 15546/20000 (77.73%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.9509, Accuracy: 11068/20000 (55.34%)\n",
            "\n",
            "Using penalty multiplier 100.0\n",
            "Train Epoch: 20 [0/20000 (0%)]\tERM loss: 0.942782\tGrad penalty: 0.000579\n",
            "First 20 logits [-1.2244016  -2.1122851   2.0319471  -1.9180826   3.289555   -0.20425624\n",
            "  2.1053681  -3.4235077  -0.24951227  0.39640608 -2.726903   -1.5959464\n",
            "  2.3132746  -0.78812814 -1.2640033   0.31024268  0.6176681   1.0077002\n",
            " -1.2567025  -0.4372559 ]\n",
            "Train Epoch: 20 [4000/20000 (20%)]\tERM loss: 0.893858\tGrad penalty: 0.000390\n",
            "First 20 logits [-0.35451153  0.45408675 -0.9439942   0.9502349  -2.4303672  -2.3910432\n",
            " -0.8444759  -0.36251405 -3.0526032  -2.426299   -0.426185    1.0756506\n",
            " -0.32435223 -0.19061095 -2.1154087   2.738076    1.457755    0.4824617\n",
            "  1.143406   -2.5932682 ]\n",
            "Train Epoch: 20 [8000/20000 (40%)]\tERM loss: 0.883597\tGrad penalty: 0.000872\n",
            "First 20 logits [-3.2473905  -1.9743983   1.1464664  -0.22584763  2.7148228  -2.2567327\n",
            "  1.588103   -0.78188545 -0.3281006  -0.65044224 -0.5254674   0.6417681\n",
            " -2.2469664   0.07828173 -2.9995282   0.8773465   0.97979724 -3.0151455\n",
            " -2.946131    1.0642834 ]\n",
            "Train Epoch: 20 [12000/20000 (60%)]\tERM loss: 0.889860\tGrad penalty: 0.001312\n",
            "First 20 logits [-3.5082881  -3.493765   -3.5676348  -2.075733    1.0226793   1.1220075\n",
            "  1.8562     -0.9206635  -0.8365186  -1.0984045   2.0525606  -2.8351188\n",
            "  3.1037226   1.445768    0.48436606 -0.44175094 -0.3051587   1.8959894\n",
            "  2.6820073   2.6916356 ]\n",
            "Train Epoch: 20 [16000/20000 (80%)]\tERM loss: 0.869099\tGrad penalty: 0.001916\n",
            "First 20 logits [-1.0865408  -0.3486002  -2.7019224   0.3991838  -0.64233786 -0.43470985\n",
            "  1.6478528  -2.1694434   1.1522292   2.1015153  -1.6722723   1.9036705\n",
            "  1.4060291  -2.0156317  -0.6405142   1.7647847  -1.1723135   3.046424\n",
            " -2.190629    2.2701538 ]\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4828, Accuracy: 15340/20000 (76.70%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.4356, Accuracy: 15867/20000 (79.33%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.0196, Accuracy: 9841/20000 (49.20%)\n",
            "\n",
            "Using penalty multiplier 100.0\n",
            "Train Epoch: 21 [0/20000 (0%)]\tERM loss: 0.934103\tGrad penalty: 0.003698\n",
            "First 20 logits [ 3.8364532  -1.5790399   1.2269312   0.68999773  0.26490602 -0.6716661\n",
            " -1.5504115  -0.69824487 -2.4251242   2.1299574  -1.8192341   1.9993055\n",
            "  1.7972128   0.02604307  0.7396844  -1.9261501   1.3108747  -0.8921255\n",
            " -1.9169942  -2.1914043 ]\n",
            "Train Epoch: 21 [4000/20000 (20%)]\tERM loss: 0.862805\tGrad penalty: 0.000155\n",
            "First 20 logits [-0.05245877 -1.968004   -2.8036654  -2.9962971  -0.00315078 -0.68419355\n",
            "  0.17582116 -1.0425798  -1.0964696   0.7587583   1.1342083   1.4741106\n",
            " -0.54175746  3.1289914   2.7911842  -0.21281925  2.6203384  -2.9048042\n",
            " -3.0338416   2.920413  ]\n",
            "Train Epoch: 21 [8000/20000 (40%)]\tERM loss: 0.869719\tGrad penalty: 0.000788\n",
            "First 20 logits [ 0.6216714  -2.6887078  -2.9805636   1.9819124  -0.07387108 -2.1165757\n",
            "  2.4909887  -1.5167028   1.0191416  -4.593917   -0.7665313   2.0996382\n",
            " -2.6904278   1.2830807  -1.4179786   2.1268249   0.20922863  2.2844114\n",
            " -1.2620322  -4.081386  ]\n",
            "Train Epoch: 21 [12000/20000 (60%)]\tERM loss: 0.893983\tGrad penalty: 0.002892\n",
            "First 20 logits [ 2.9175088  -3.491331   -1.167623   -1.3241869  -3.2050383   0.48551658\n",
            "  0.487771    1.8343236   1.1496358  -1.7211329  -0.40893367  0.96885985\n",
            "  0.55484086 -0.68407756 -2.7318468   1.0491875  -1.1372414   0.7833521\n",
            " -1.1053671   1.8845482 ]\n",
            "Train Epoch: 21 [16000/20000 (80%)]\tERM loss: 0.950389\tGrad penalty: 0.000442\n",
            "First 20 logits [-0.32187527  1.3446807  -2.696921    2.1548672  -0.8210438   0.18182787\n",
            " -1.6469687  -1.591069    0.4100545  -1.7196654  -2.466478    0.21690899\n",
            "  1.5958039   1.125024    1.4252412   3.286752    1.2960808   2.6956728\n",
            " -0.01452233 -1.896093  ]\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5162, Accuracy: 15013/20000 (75.06%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.4947, Accuracy: 15114/20000 (75.57%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.8756, Accuracy: 11790/20000 (58.95%)\n",
            "\n",
            "Using penalty multiplier 110.0\n",
            "Train Epoch: 22 [0/20000 (0%)]\tERM loss: 0.995325\tGrad penalty: 0.002246\n",
            "First 20 logits [ 0.18982026  2.7034428  -2.454015    0.62110454 -2.6537924   3.0218136\n",
            "  1.1820464  -0.7469241   0.21002302  0.02879194 -0.48840314 -1.8536817\n",
            " -1.5835435   0.28539088  3.585157    3.2715652   1.7755852  -2.7483244\n",
            "  1.9657073   3.348737  ]\n",
            "Train Epoch: 22 [4000/20000 (20%)]\tERM loss: 0.942082\tGrad penalty: 0.000594\n",
            "First 20 logits [-1.5312318   2.8017168  -2.3769503   0.45982972  1.881071   -1.1784903\n",
            " -2.1460989   1.7270111   2.6040134  -0.49011678  0.22914867 -2.4096754\n",
            " -1.1716213  -0.28331476 -0.33168715  2.4656556  -1.8388883  -0.22721471\n",
            " -0.01439402 -0.78043157]\n",
            "Train Epoch: 22 [8000/20000 (40%)]\tERM loss: 0.960792\tGrad penalty: -0.000109\n",
            "First 20 logits [-2.8084586  -1.5868778  -1.3218938  -0.06790731 -0.66338307  0.55677426\n",
            "  1.8546021  -0.9576047   2.5691078  -2.9955935  -2.18795     1.9699366\n",
            " -0.0573228   0.284756   -1.3128023  -2.0815349   0.38374642 -0.9434554\n",
            " -3.7881982   1.8602786 ]\n",
            "Train Epoch: 22 [12000/20000 (60%)]\tERM loss: 0.904013\tGrad penalty: 0.001808\n",
            "First 20 logits [-0.20330319 -1.282109   -2.0219822   1.3226764   1.5288539   0.03611772\n",
            " -3.366738    1.9402614   2.5132763  -0.65995693 -2.8747966  -3.2509358\n",
            " -0.94796187 -1.0726155   0.40521905  0.7464777  -0.8731541   1.6447526\n",
            "  2.0790203  -2.1881661 ]\n",
            "Train Epoch: 22 [16000/20000 (80%)]\tERM loss: 0.878345\tGrad penalty: 0.003496\n",
            "First 20 logits [-2.5155842  -1.0995616   0.44743395  0.11466594  1.3967667  -1.873505\n",
            "  0.5201453  -1.7730922  -1.8270057  -3.3263638  -0.53860855 -0.9303725\n",
            " -3.2050188  -2.7750869   2.734115    0.11208037 -2.213606   -2.2323375\n",
            " -0.34598002  2.1926525 ]\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4686, Accuracy: 15541/20000 (77.70%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.4049, Accuracy: 16099/20000 (80.50%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.0961, Accuracy: 9276/20000 (46.38%)\n",
            "\n",
            "Using penalty multiplier 110.0\n",
            "Train Epoch: 23 [0/20000 (0%)]\tERM loss: 0.857377\tGrad penalty: 0.001385\n",
            "First 20 logits [-0.24642123 -2.7170124  -0.75869304 -2.8653924   0.68957406 -2.879807\n",
            " -3.132203   -3.006264    1.2189261   0.18925484 -0.46431175 -3.3048713\n",
            "  2.6394608  -0.12511158  0.46263394  2.2032225  -2.4175265  -2.9241745\n",
            " -3.000373   -0.15779445]\n",
            "Train Epoch: 23 [4000/20000 (20%)]\tERM loss: 0.866117\tGrad penalty: 0.000537\n",
            "First 20 logits [-1.1676316   0.05400639 -1.2149284   1.8394607  -1.6081365  -1.1401064\n",
            "  1.1922233  -2.3036718  -1.7825485  -2.7115414   1.1242591  -3.0621352\n",
            " -0.3563442  -1.1123511   1.3060329   1.6221648   1.6591035   1.987628\n",
            "  1.3186266  -2.139336  ]\n",
            "Train Epoch: 23 [8000/20000 (40%)]\tERM loss: 0.895188\tGrad penalty: 0.000596\n",
            "First 20 logits [ 0.06201322  0.75196356 -0.17027491  3.1370168   3.3446448  -2.044429\n",
            "  2.1861844   0.25872397 -1.6420997  -0.9192448   1.3621651  -0.40512267\n",
            " -2.9363484   1.4891688  -1.0830544   1.1987085  -0.83107793 -2.7848358\n",
            " -0.8668665   2.6885734 ]\n",
            "Train Epoch: 23 [12000/20000 (60%)]\tERM loss: 0.928375\tGrad penalty: -0.001358\n",
            "First 20 logits [-3.3415759   2.6326423   2.8924224   0.565657    0.22180067  3.8365939\n",
            " -0.04479061  2.296266    2.7599359   2.41671    -2.3211255  -0.35268626\n",
            " -1.4570112   1.699092    0.18747868  3.0457976   1.6104261  -1.7187011\n",
            "  0.8129917   0.06900211]\n",
            "Train Epoch: 23 [16000/20000 (80%)]\tERM loss: 0.924899\tGrad penalty: 0.000813\n",
            "First 20 logits [ 3.538871    0.10350691  2.190826    2.960356   -0.9459271  -2.1785731\n",
            "  0.37252098 -1.2172377   2.8188257   0.5129927  -2.4108822   2.0409641\n",
            " -1.6840757   1.6733854   3.0038893  -1.2944349  -0.6551885  -1.5606122\n",
            " -0.77301544 -2.8937292 ]\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4888, Accuracy: 15206/20000 (76.03%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.4561, Accuracy: 15447/20000 (77.23%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.8837, Accuracy: 11287/20000 (56.44%)\n",
            "\n",
            "Using penalty multiplier 120.0\n",
            "Train Epoch: 24 [0/20000 (0%)]\tERM loss: 0.941375\tGrad penalty: 0.000535\n",
            "First 20 logits [ 1.945579    0.9073126   0.34956926  1.3376815  -1.7871896   2.5072083\n",
            "  2.4227567  -0.24203041 -1.5109988  -0.6392206  -1.6199703  -1.3297591\n",
            "  0.75774336  0.6678653  -1.091671   -1.2454065   2.0996978  -2.3814516\n",
            " -1.6450804   1.197125  ]\n",
            "Train Epoch: 24 [4000/20000 (20%)]\tERM loss: 0.960740\tGrad penalty: -0.000031\n",
            "First 20 logits [-1.4999658  -0.94708425  2.592477    3.2337034   2.3877428   0.4703024\n",
            "  0.0657527   1.9287181   2.3400774  -1.8067772  -2.1503077  -0.36457187\n",
            " -2.0241299   1.5380253   1.7076533  -1.6545863   2.209783    1.3032895\n",
            "  0.09535673 -2.186878  ]\n",
            "Train Epoch: 24 [8000/20000 (40%)]\tERM loss: 0.965705\tGrad penalty: 0.000108\n",
            "First 20 logits [-2.5554934e+00  5.1417083e-01  2.6025281e+00  1.5144083e+00\n",
            " -3.7203053e-01  9.2196554e-01  1.8737696e+00 -1.6056957e+00\n",
            "  4.1921220e+00  3.8154669e+00  1.1986982e+00 -1.7799493e+00\n",
            "  3.5404953e-01  1.2795460e-01  1.3746189e-01 -3.9916873e-01\n",
            " -3.7696841e-03  4.4834755e-02  3.9496914e-01  2.6595750e+00]\n",
            "Train Epoch: 24 [12000/20000 (60%)]\tERM loss: 0.927143\tGrad penalty: 0.000293\n",
            "First 20 logits [-0.6018711  -0.902807    2.1585042  -0.22883202 -0.3349916  -0.4482158\n",
            "  1.0093036  -2.2709234   0.5577011  -1.2390362   1.73518     2.1675558\n",
            " -1.1164205   2.5419405   1.1757902  -1.705246   -0.2356055  -0.0403192\n",
            "  2.0272012   1.9005598 ]\n",
            "Train Epoch: 24 [16000/20000 (80%)]\tERM loss: 0.910696\tGrad penalty: -0.000130\n",
            "First 20 logits [-2.4928687   0.63450724 -1.0177325   2.2910187   0.5029849   2.519917\n",
            "  0.7935751  -1.1890031  -1.750479   -2.2411096  -0.7052182   1.9427874\n",
            " -2.1618586   0.7062029  -1.5039902  -2.1505008  -0.79407966  2.3237042\n",
            "  1.8346754   1.4048768 ]\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4715, Accuracy: 15405/20000 (77.03%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.4307, Accuracy: 15627/20000 (78.14%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.9644, Accuracy: 10891/20000 (54.45%)\n",
            "\n",
            "Using penalty multiplier 120.0\n",
            "Train Epoch: 25 [0/20000 (0%)]\tERM loss: 0.890945\tGrad penalty: -0.000444\n",
            "First 20 logits [ 2.0509887  -1.8413913   2.0430377   1.9746624  -3.2426238  -0.7278958\n",
            "  1.7753608  -0.39986855 -0.99781394 -2.632014    0.52772963  1.8942635\n",
            "  0.44528785  1.2613769  -0.5347406   3.1698453  -2.745772   -1.030758\n",
            "  0.6886827  -1.4339719 ]\n",
            "Train Epoch: 25 [4000/20000 (20%)]\tERM loss: 0.879207\tGrad penalty: 0.001604\n",
            "First 20 logits [-2.9648185  -2.9902172  -1.2126333  -0.3838193   3.329289    2.7970405\n",
            "  1.6637046   2.2388117  -0.00750396 -2.5518072  -0.54371196  2.7461195\n",
            "  2.8131294   0.07447024 -0.92723    -2.037286   -1.9499722   1.8916007\n",
            "  0.7670583  -2.993918  ]\n",
            "Train Epoch: 25 [8000/20000 (40%)]\tERM loss: 0.859491\tGrad penalty: 0.001431\n",
            "First 20 logits [ 1.2250259 -0.3363483 -2.113242   1.9388529 -2.3642714  1.0944793\n",
            " -3.1598961 -3.1600952 -2.2261405 -1.0173783 -1.7223392 -2.5241432\n",
            " -2.519651   0.7733234 -0.4305166  2.5842617  2.9515133 -0.9144968\n",
            " -3.5551648 -2.2720108]\n",
            "Train Epoch: 25 [12000/20000 (60%)]\tERM loss: 0.891787\tGrad penalty: 0.000251\n",
            "First 20 logits [-3.3499434   3.0529203   1.8622154  -1.8902858  -1.3137151   3.4624054\n",
            " -0.23108791 -3.0497787  -1.9327208  -2.732861   -3.4900331   0.01078149\n",
            " -0.44336495 -2.7068777  -1.9769926  -3.1827934  -2.4757173   2.2582178\n",
            " -1.9759562   1.0461192 ]\n",
            "Train Epoch: 25 [16000/20000 (80%)]\tERM loss: 0.838987\tGrad penalty: -0.001124\n",
            "First 20 logits [-2.6893253  -0.63703656 -0.25853404  2.6433105  -2.9279556  -1.1726478\n",
            "  0.6603089  -0.11086541 -1.1373899  -1.7376685   2.0311177   0.72666836\n",
            " -3.7034812  -1.331845   -2.452888   -2.414322   -1.7510229   2.8380296\n",
            " -3.0206912  -0.85839784]\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4595, Accuracy: 15557/20000 (77.78%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.4032, Accuracy: 15968/20000 (79.84%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.0668, Accuracy: 9826/20000 (49.13%)\n",
            "\n",
            "Using penalty multiplier 130.0\n",
            "Train Epoch: 26 [0/20000 (0%)]\tERM loss: 0.842061\tGrad penalty: 0.000068\n",
            "First 20 logits [ 2.6320112  -0.31415364 -2.6027968  -3.5825572  -3.2229736   2.5032294\n",
            "  0.18126602  3.444365   -2.5069487  -1.790021    3.0124123   1.4155414\n",
            "  2.5452268   0.5346152   2.1163151  -2.7633207   0.22687109 -2.3096607\n",
            " -0.9349253  -2.745449  ]\n",
            "Train Epoch: 26 [4000/20000 (20%)]\tERM loss: 0.891193\tGrad penalty: 0.000200\n",
            "First 20 logits [ 0.23946205 -1.9172182  -2.4920778   0.35265008  3.4694314  -0.37650192\n",
            " -1.1893511  -0.2911282   4.2918777   2.3883188  -3.3835227  -0.5116001\n",
            " -2.8959253  -1.7414058  -1.9147005  -2.2644928  -3.0709157   0.98292166\n",
            " -0.03824925 -0.38099205]\n",
            "Train Epoch: 26 [8000/20000 (40%)]\tERM loss: 0.862890\tGrad penalty: 0.000131\n",
            "First 20 logits [-2.4491246   2.6624534   3.2234058  -3.3233662   2.5851958  -0.0493192\n",
            "  1.1055762  -2.8225033   3.8970013   1.6361194   0.18085642 -2.5479121\n",
            " -1.2885444  -1.5980616  -1.9196484  -1.048059   -0.9302443  -0.44923118\n",
            "  3.188093    3.2092016 ]\n",
            "Train Epoch: 26 [12000/20000 (60%)]\tERM loss: 0.920735\tGrad penalty: 0.002805\n",
            "First 20 logits [ 8.5653633e-01 -3.0633206e+00 -9.1032791e-01 -3.6461875e+00\n",
            " -3.0198460e+00  2.7364290e+00 -2.7639925e+00 -7.1136631e-02\n",
            " -2.3433850e+00  5.6409019e-01  1.6421474e+00  5.7619303e-01\n",
            "  2.2744458e+00 -3.3008399e+00 -2.8293533e-03 -1.9560666e+00\n",
            "  2.0664531e-01 -1.7244011e-02  2.3026497e+00 -1.0156954e-01]\n",
            "Train Epoch: 26 [16000/20000 (80%)]\tERM loss: 0.913830\tGrad penalty: 0.002706\n",
            "First 20 logits [ 0.8574128   2.0537992  -1.3063937   2.3019183   0.6371028  -0.54920405\n",
            " -0.9700682  -1.4438537   0.90439415  1.7850499   0.31563395 -2.7688797\n",
            "  3.3799815   1.3128262   0.39668947 -0.84411466  2.373529    0.26082018\n",
            "  0.5507351  -1.2817656 ]\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4890, Accuracy: 15180/20000 (75.90%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.4673, Accuracy: 15239/20000 (76.19%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.8531, Accuracy: 12218/20000 (61.09%)\n",
            "\n",
            "found acceptable values. stopping training.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGh00cHNxpPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}